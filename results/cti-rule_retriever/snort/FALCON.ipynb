{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "745c7ad1",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1740d57e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ceed0e",
      "metadata": {},
      "source": [
        "# Path Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5ed706f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
        "project_base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dce6fdad",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON/data/evaluation/cti-rule/snort/cti_snort_eval_data.pkl'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cti_snort_eval_data_path = os.path.join(project_base_path, \"data/evaluation/cti-rule/snort/cti_snort_eval_data.pkl\")\n",
        "cti_snort_eval_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053fdc9f",
      "metadata": {},
      "source": [
        "# Misc Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f1fa95b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_from_pickle(file_path):\n",
        "    \"\"\"\n",
        "    Loads data from a pickle file.\n",
        "\n",
        "    :param file_path: Path to the pickle file\n",
        "    :return: Loaded data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from pickle: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be1ce081",
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_subset_indices(full_list, subset_list):\n",
        "    \"\"\"\n",
        "    Maps each string in the subset list to its index in the full list.\n",
        "\n",
        "    Args:\n",
        "        full_list (list of str): The complete list of strings.\n",
        "        subset_list (list of str): A subset of strings present in the full list.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with subset strings as keys and their indices in the full list as values.\n",
        "    \"\"\"\n",
        "    index_map = {}\n",
        "    for item in subset_list:\n",
        "        try:\n",
        "            index_map[item] = full_list.index(item)\n",
        "        except ValueError:\n",
        "            # Just in case the subset contains a string not found in full_list\n",
        "            index_map[item] = -1\n",
        "    return index_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "27d32c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_topk_match(gt_indices, sorted_pred_indices, top_k):\n",
        "    top_k_preds = set(sorted_pred_indices[:top_k])\n",
        "    matched = top_k_preds.intersection(set(gt_indices))\n",
        "    return 100 * len(matched) / len(gt_indices) if gt_indices else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "81be3a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reciprocal_rank(gt_indices, sorted_pred_indices):\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            return 1.0 / rank\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c2ff1f9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_precision(gt_indices, sorted_pred_indices):\n",
        "    hits, score = 0, 0.0\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            hits += 1\n",
        "            score += hits / rank\n",
        "    return score / len(gt_indices) if gt_indices else 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8981d174",
      "metadata": {},
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8813b9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ⚙️ Config\n",
        "FINE_TUNED_MODEL_NAME = \"all-mpnet-base-v2\"\n",
        "MODEL_NAME = f\"/data/common/models/sentence-transformers/{FINE_TUNED_MODEL_NAME}\"\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "RUN = 2\n",
        "###########################\n",
        "MAX_LEN = 512\n",
        "FINE_TUNED_MODEL_STATE_NAME = f\"contrastive_encoder_r{RUN}.pt\"\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():  \n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "        \n",
        "MODEL_LOAD_PATH = os.path.join(project_base_path, f\"script/fine_tuning/bi-encoder/snort/{FINE_TUNED_MODEL_NAME}/{FINE_TUNED_MODEL_STATE_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "346b52a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bi-Encoder Model\n",
        "class SentenceEncoder(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "        return nn.functional.normalize(embeddings, p=2, dim=1)  # Normalize for cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7aca813f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model\n",
        "model = SentenceEncoder(MODEL_NAME).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffce733",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa929272",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1315c3f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "802\n"
          ]
        }
      ],
      "source": [
        "# Load the data back from the pickle file\n",
        "cti_snort_eval_data = load_from_pickle(cti_snort_eval_data_path)\n",
        "print(len(cti_snort_eval_data.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ae3973",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b774db9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "consolidated_dummy_snort_rules = []\n",
        "for cti, rules in cti_snort_eval_data.items():\n",
        "    consolidated_dummy_snort_rules.extend(rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8aff5748",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3063"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(consolidated_dummy_snort_rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b8922cf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ctis = list(cti_snort_eval_data.keys())\n",
        "total_recall = total_map =  0\n",
        "recall_k_list = []\n",
        "map_score_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9867e6",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "887e4900",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-Snort Semantic Scorer: 100%|██████████| 802/802 [1:15:26<00:00,  5.64s/it]\n"
          ]
        }
      ],
      "source": [
        "test_ctis = list(cti_snort_eval_data.keys())\n",
        "\n",
        "total_recall, total_map = 0, 0\n",
        "recall_k_list = []\n",
        "map_score_list = []\n",
        "\n",
        "for cti in tqdm(test_ctis, \"Evaluating CTI-Snort Semantic Scorer\"):\n",
        "    result_idx = map_subset_indices(consolidated_dummy_snort_rules, cti_snort_eval_data[cti])\n",
        "    gt_indices = list(result_idx.values())\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        tokenized_cti = tokenizer(cti, return_tensors=\"pt\", padding=True, max_length=MAX_LEN, truncation=True)\n",
        "        tokenized_dummy_rules = tokenizer(consolidated_dummy_snort_rules, return_tensors=\"pt\", padding=True, max_length=MAX_LEN, truncation=True)\n",
        "        \n",
        "        input_ids_tokenized_cti = tokenized_cti['input_ids'].to(DEVICE)\n",
        "        attention_mask_tokenized_cti = tokenized_cti['attention_mask'].to(DEVICE)\n",
        "        input_ids_tokenized_dummy_rules = tokenized_dummy_rules['input_ids'].to(DEVICE)\n",
        "        attention_mask_tokenized_dummy_rules = tokenized_dummy_rules['attention_mask'].to(DEVICE)\n",
        "        \n",
        "        emb_tokenized_cti = model(input_ids_tokenized_cti, attention_mask_tokenized_cti)\n",
        "        emb_tokenized_dummy_rules = model(input_ids_tokenized_dummy_rules, attention_mask_tokenized_dummy_rules)\n",
        "       \n",
        "        dot_product_matrix = torch.matmul(emb_tokenized_cti, emb_tokenized_dummy_rules.T)\n",
        "        \n",
        "        similarity_scores = dot_product_matrix[0]\n",
        "        sorted_indices = torch.argsort(similarity_scores, descending=True).tolist()\n",
        "        \n",
        "        # print(sorted_indices)\n",
        "        # print(gt_indices)\n",
        "        \n",
        "        # Metric Calculations\n",
        "        k = len(gt_indices)\n",
        "        recall_k = evaluate_topk_match(gt_indices, sorted_indices, 10)\n",
        "        recall_k_list.append(recall_k)\n",
        "        map_score = average_precision(gt_indices, sorted_indices)\n",
        "        map_score_list.append(map_score)\n",
        "\n",
        "        # Accumulate\n",
        "        total_recall += recall_k\n",
        "        total_map += map_score\n",
        "\n",
        "        # Print for each CTI if needed\n",
        "        # print(f\"\\nCTI: {cti[:50]}...\")\n",
        "        # print(f\"GT Indices: {gt_indices}\")\n",
        "        # print(f\"Top-{k} Recall: {recall_k:.2f}%, MAP: {map_score:.4f}\")\n",
        "    \n",
        "    # Free up memory\n",
        "    del emb_tokenized_cti, emb_tokenized_dummy_rules\n",
        "    del input_ids_tokenized_cti, attention_mask_tokenized_cti\n",
        "    del input_ids_tokenized_dummy_rules, attention_mask_tokenized_dummy_rules\n",
        "    torch.cuda.empty_cache()   # Clear GPU memory "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b1205c",
      "metadata": {},
      "source": [
        "# Top - K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6c740a",
      "metadata": {},
      "source": [
        "## all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee82c679",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50836906",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c000f24",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c6cc24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e33616c",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1782dd34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3136418",
      "metadata": {},
      "source": [
        "## all-mpnet-base-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe80f23e",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "89e00048",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 24.45%\n",
            "Mean Average Precision (MAP): 0.2681\n",
            "Recall@K Standard Deviation: 26.4152\n",
            "MAP Standard Deviation: 0.2764\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fff93ff",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "26604c97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 25.46%\n",
            "Mean Average Precision (MAP): 0.2777\n",
            "Recall@K Standard Deviation: 26.7962\n",
            "MAP Standard Deviation: 0.2778\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d550dc",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1de0cd41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 23.95%\n",
            "Mean Average Precision (MAP): 0.2572\n",
            "Recall@K Standard Deviation: 26.9702\n",
            "MAP Standard Deviation: 0.2764\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750a16ea",
      "metadata": {},
      "source": [
        "# Top - 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7203a551",
      "metadata": {},
      "source": [
        "## all-MiniLM-L6-v2 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96170c1",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b3c8be12",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 34.98%\n",
            "Mean Average Precision (MAP): 0.2672\n",
            "Recall@K Standard Deviation: 32.6833\n",
            "MAP Standard Deviation: 0.2735\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3eaf8c9",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aed06a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c7e8c3",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "046acc3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 35.77%\n",
            "Mean Average Precision (MAP): 0.2824\n",
            "Recall@K Standard Deviation: 32.0971\n",
            "MAP Standard Deviation: 0.2782\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6530d4cc",
      "metadata": {},
      "source": [
        "## all-mpnet-base-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb3fd6a",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b58f69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49dc6c1",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09255f61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6028a91a",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ae1c163d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 32.87%\n",
            "Mean Average Precision (MAP): 0.2572\n",
            "Recall@K Standard Deviation: 32.1238\n",
            "MAP Standard Deviation: 0.2764\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddc6ec4",
      "metadata": {},
      "source": [
        "## e5-base-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6de3d61",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0391c84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661dc74d",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbb5429",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019a9dad",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c76c46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790729df",
      "metadata": {},
      "source": [
        "# Top - 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dee6435",
      "metadata": {},
      "source": [
        "## all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4571539",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ceeb4908",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 42.69%\n",
            "Mean Average Precision (MAP): 0.2672\n",
            "Recall@K Standard Deviation: 34.4496\n",
            "MAP Standard Deviation: 0.2735\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde6fa24",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b20eb504",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 43.67%\n",
            "Mean Average Precision (MAP): 0.2796\n",
            "Recall@K Standard Deviation: 33.8329\n",
            "MAP Standard Deviation: 0.2788\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c9adad",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ce056d71",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 42.91%\n",
            "Mean Average Precision (MAP): 0.2824\n",
            "Recall@K Standard Deviation: 33.8234\n",
            "MAP Standard Deviation: 0.2782\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8f3f6f",
      "metadata": {},
      "source": [
        "# Top - 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958d0b1f",
      "metadata": {},
      "source": [
        "## all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075ccde3",
      "metadata": {},
      "source": [
        "### Run - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92f9eee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a763dfb9",
      "metadata": {},
      "source": [
        "### Run - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b51059",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b6805e",
      "metadata": {},
      "source": [
        "### Run - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f06d4b11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 52.10%\n",
            "Mean Average Precision (MAP): 0.2824\n",
            "Recall@K Standard Deviation: 34.8825\n",
            "MAP Standard Deviation: 0.2782\n"
          ]
        }
      ],
      "source": [
        "# Final Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "    \n",
        "# Standard Deviation of Scores\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b083c8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
