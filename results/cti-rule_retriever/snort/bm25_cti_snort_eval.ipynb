{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BM25-Based Evaluation for CTI vs. Snort Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“¦ Imports\n",
        "import os\n",
        "import pickle\n",
        "import statistics\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6ab1728",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_from_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Adjust paths as needed\n",
        "project_base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
        "project_base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4de40f01",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "802"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cti_snort_eval_data_path = os.path.join(project_base_path, \"data/evaluation/cti-rule/snort/cti_snort_eval_data.pkl\")\n",
        "cti_snort_eval_data = load_from_pickle(cti_snort_eval_data_path)\n",
        "test_ctis = list(cti_snort_eval_data.keys())\n",
        "len(test_ctis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def map_subset_indices(full_list, subset_list):\n",
        "    index_map = {}\n",
        "    for item in subset_list:\n",
        "        try:\n",
        "            index_map[item] = full_list.index(item)\n",
        "        except ValueError:\n",
        "            index_map[item] = -1\n",
        "    return index_map\n",
        "\n",
        "def evaluate_topk_match(gt_indices, sorted_pred_indices, top_k):\n",
        "    top_k_preds = set(sorted_pred_indices[:top_k])\n",
        "    matched = top_k_preds.intersection(set(gt_indices))\n",
        "    return 100 * len(matched) / len(gt_indices) if gt_indices else 0\n",
        "\n",
        "def average_precision(gt_indices, sorted_pred_indices):\n",
        "    hits, score = 0, 0.0\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            hits += 1\n",
        "            score += hits / rank\n",
        "    return score / len(gt_indices) if gt_indices else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  BM25 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-Snort with BM25:   0%|          | 2/802 [00:00<00:55, 14.31it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-Snort with BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 802/802 [00:59<00:00, 13.54it/s]\n"
          ]
        }
      ],
      "source": [
        "consolidated_dummy_snort_rules = []\n",
        "for rules in cti_snort_eval_data.values():\n",
        "    consolidated_dummy_snort_rules.extend(rules)\n",
        "\n",
        "tokenized_corpus = [tokenize(rule) for rule in consolidated_dummy_snort_rules]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "total_recall, total_map = 0, 0\n",
        "recall_k_list = []\n",
        "map_score_list = []\n",
        "\n",
        "for cti in tqdm(test_ctis, desc=\"Evaluating CTI-Snort with BM25\"):\n",
        "    result_idx = map_subset_indices(consolidated_dummy_snort_rules, cti_snort_eval_data[cti])\n",
        "    gt_indices = list(result_idx.values())\n",
        "\n",
        "    tokenized_cti = tokenize(cti)\n",
        "    scores = bm25.get_scores(tokenized_cti)\n",
        "    sorted_indices = np.argsort(scores)[::-1].tolist()\n",
        "\n",
        "    k = len(gt_indices)\n",
        "    recall_k = evaluate_topk_match(gt_indices, sorted_indices, top_k=50)\n",
        "    map_score = average_precision(gt_indices, sorted_indices)\n",
        "\n",
        "    recall_k_list.append(recall_k)\n",
        "    map_score_list.append(map_score)\n",
        "\n",
        "    total_recall += recall_k\n",
        "    total_map += map_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Evaluation Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a33662",
      "metadata": {},
      "source": [
        "### Top - K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BM25 Evaluation Results ===\n",
            "Average Recall@K: 23.71%\n",
            "Mean Average Precision (MAP): 0.2541\n",
            "Recall@K Standard Deviation: 26.5647\n",
            "MAP Standard Deviation: 0.2621\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== BM25 Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bc7b86",
      "metadata": {},
      "source": [
        "### Top - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "db7fd299",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BM25 Evaluation Results ===\n",
            "Average Recall@K: 33.23%\n",
            "Mean Average Precision (MAP): 0.2541\n",
            "Recall@K Standard Deviation: 30.7656\n",
            "MAP Standard Deviation: 0.2621\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== BM25 Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c5db5b",
      "metadata": {},
      "source": [
        "### Top - 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cdc5125c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BM25 Evaluation Results ===\n",
            "Average Recall@K: 41.52%\n",
            "Mean Average Precision (MAP): 0.2541\n",
            "Recall@K Standard Deviation: 32.8795\n",
            "MAP Standard Deviation: 0.2621\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== BM25 Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc8b51c",
      "metadata": {},
      "source": [
        "### Top - 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "af20d3d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BM25 Evaluation Results ===\n",
            "Average Recall@K: 53.30%\n",
            "Mean Average Precision (MAP): 0.2541\n",
            "Recall@K Standard Deviation: 33.9624\n",
            "MAP Standard Deviation: 0.2621\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== BM25 Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93692e40",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
