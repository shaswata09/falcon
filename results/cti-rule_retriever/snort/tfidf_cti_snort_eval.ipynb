{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF + Cosine Similarity Evaluation for CTI vs. Snort Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“¦ Imports\n",
        "import os\n",
        "import pickle\n",
        "import statistics\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_from_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Adjust paths as needed\n",
        "project_base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
        "project_base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a005e76b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "802"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cti_snort_eval_data_path = os.path.join(project_base_path, \"data/evaluation/cti-rule/snort/cti_snort_eval_data.pkl\")\n",
        "cti_snort_eval_data = load_from_pickle(cti_snort_eval_data_path)\n",
        "test_ctis = list(cti_snort_eval_data.keys())\n",
        "len(test_ctis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_subset_indices(full_list, subset_list):\n",
        "    index_map = {}\n",
        "    for item in subset_list:\n",
        "        try:\n",
        "            index_map[item] = full_list.index(item)\n",
        "        except ValueError:\n",
        "            index_map[item] = -1\n",
        "    return index_map\n",
        "\n",
        "def evaluate_topk_match(gt_indices, sorted_pred_indices, top_k):\n",
        "    top_k_preds = set(sorted_pred_indices[:top_k])\n",
        "    matched = top_k_preds.intersection(set(gt_indices))\n",
        "    return 100 * len(matched) / len(gt_indices) if gt_indices else 0\n",
        "\n",
        "def average_precision(gt_indices, sorted_pred_indices):\n",
        "    hits, score = 0, 0.0\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            hits += 1\n",
        "            score += hits / rank\n",
        "    return score / len(gt_indices) if gt_indices else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  TF-IDF Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-Snort with TF-IDF:   0%|          | 0/802 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-Snort with TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 802/802 [00:01<00:00, 741.01it/s]\n"
          ]
        }
      ],
      "source": [
        "consolidated_dummy_snort_rules = []\n",
        "for rules in cti_snort_eval_data.values():\n",
        "    consolidated_dummy_snort_rules.extend(rules)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(consolidated_dummy_snort_rules)\n",
        "\n",
        "total_recall, total_map = 0, 0\n",
        "recall_k_list = []\n",
        "map_score_list = []\n",
        "\n",
        "for cti in tqdm(test_ctis, desc=\"Evaluating CTI-Snort with TF-IDF\"):\n",
        "    result_idx = map_subset_indices(consolidated_dummy_snort_rules, cti_snort_eval_data[cti])\n",
        "    gt_indices = list(result_idx.values())\n",
        "\n",
        "    tfidf_cti = vectorizer.transform([cti])\n",
        "    cosine_scores = cosine_similarity(tfidf_cti, tfidf_matrix).flatten()\n",
        "    sorted_indices = np.argsort(cosine_scores)[::-1].tolist()\n",
        "\n",
        "    k = len(gt_indices)\n",
        "    recall_k = evaluate_topk_match(gt_indices, sorted_indices, top_k=k)\n",
        "    map_score = average_precision(gt_indices, sorted_indices)\n",
        "\n",
        "    recall_k_list.append(recall_k)\n",
        "    map_score_list.append(map_score)\n",
        "\n",
        "    total_recall += recall_k\n",
        "    total_map += map_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Evaluation Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f785bc",
      "metadata": {},
      "source": [
        "### Top - k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TF-IDF Evaluation Results ===\n",
            "Average Recall@K: 23.00%\n",
            "Mean Average Precision (MAP): 0.2512\n",
            "Recall@K Standard Deviation: 26.0899\n",
            "MAP Standard Deviation: 0.2557\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== TF-IDF Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc159edc",
      "metadata": {},
      "source": [
        "### Top - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "59d43d42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TF-IDF Evaluation Results ===\n",
            "Average Recall@K: 33.92%\n",
            "Mean Average Precision (MAP): 0.2512\n",
            "Recall@K Standard Deviation: 30.5886\n",
            "MAP Standard Deviation: 0.2557\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== TF-IDF Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada5d3b9",
      "metadata": {},
      "source": [
        "### Top - 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "aa1095fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TF-IDF Evaluation Results ===\n",
            "Average Recall@K: 42.39%\n",
            "Mean Average Precision (MAP): 0.2512\n",
            "Recall@K Standard Deviation: 32.5518\n",
            "MAP Standard Deviation: 0.2557\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== TF-IDF Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e892ef8",
      "metadata": {},
      "source": [
        "### Top - 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6326284c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TF-IDF Evaluation Results ===\n",
            "Average Recall@K: 55.20%\n",
            "Mean Average Precision (MAP): 0.2512\n",
            "Recall@K Standard Deviation: 33.5142\n",
            "MAP Standard Deviation: 0.2557\n"
          ]
        }
      ],
      "source": [
        "n = len(test_ctis)\n",
        "print(\"\\n=== TF-IDF Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "print(f\"Recall@K Standard Deviation: {statistics.stdev(recall_k_list):.4f}\")\n",
        "print(f\"MAP Standard Deviation: {statistics.stdev(map_score_list):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ea6021",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
