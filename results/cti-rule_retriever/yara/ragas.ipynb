{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "745c7ad1",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1740d57e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ceed0e",
      "metadata": {},
      "source": [
        "# Path Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5ed706f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
        "project_base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dce6fdad",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON/data/evaluation/cti-rule/yara/cti_yara_eval_data.pkl'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cti_yara_eval_data_path = os.path.join(project_base_path, \"data/evaluation/cti-rule/yara/cti_yara_eval_data.pkl\")\n",
        "cti_yara_eval_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053fdc9f",
      "metadata": {},
      "source": [
        "# Misc Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1fa95b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_from_pickle(file_path):\n",
        "    \"\"\"\n",
        "    Loads data from a pickle file.\n",
        "\n",
        "    :param file_path: Path to the pickle file\n",
        "    :return: Loaded data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from pickle: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be1ce081",
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_subset_indices(full_list, subset_list):\n",
        "    \"\"\"\n",
        "    Maps each string in the subset list to its index in the full list.\n",
        "\n",
        "    Args:\n",
        "        full_list (list of str): The complete list of strings.\n",
        "        subset_list (list of str): A subset of strings present in the full list.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with subset strings as keys and their indices in the full list as values.\n",
        "    \"\"\"\n",
        "    index_map = {}\n",
        "    for item in subset_list:\n",
        "        try:\n",
        "            index_map[item] = full_list.index(item)\n",
        "        except ValueError:\n",
        "            # Just in case the subset contains a string not found in full_list\n",
        "            index_map[item] = -1\n",
        "    return index_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27d32c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_topk_match(gt_indices, sorted_pred_indices, top_k):\n",
        "    top_k_preds = set(sorted_pred_indices[:top_k])\n",
        "    matched = top_k_preds.intersection(set(gt_indices))\n",
        "    return 100 * len(matched) / len(gt_indices) if gt_indices else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "81be3a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reciprocal_rank(gt_indices, sorted_pred_indices):\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            return 1.0 / rank\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c2ff1f9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_precision(gt_indices, sorted_pred_indices):\n",
        "    hits, score = 0, 0.0\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            hits += 1\n",
        "            score += hits / rank\n",
        "    return score / len(gt_indices) if gt_indices else 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8981d174",
      "metadata": {},
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4c351478",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "open_ai_key = \"OPENAI_KEY\"\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffce733",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa929272",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1315c3f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "916\n"
          ]
        }
      ],
      "source": [
        "# Load the data back from the pickle file\n",
        "cti_yara_eval_data = load_from_pickle(cti_yara_eval_data_path)\n",
        "print(len(cti_yara_eval_data.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ae3973",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b774db9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "consolidated_dummy_yara_rules = []\n",
        "for cti, rules in cti_yara_eval_data.items():\n",
        "    consolidated_dummy_yara_rules.extend(rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8aff5748",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5106"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(consolidated_dummy_yara_rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9867e6",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f5840e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bb45e5cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAG:\n",
        "    def __init__(self, model=\"gpt-4o\"):\n",
        "        self.llm = ChatOpenAI(model=model)\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.doc_embeddings = None\n",
        "        self.docs = None\n",
        "\n",
        "    def load_documents(self, documents):\n",
        "        self.docs = documents\n",
        "        self.doc_embeddings = self.embeddings.embed_documents(documents)\n",
        "\n",
        "    def get_top_k_docs(self, query, k=10):\n",
        "        if not self.docs or not self.doc_embeddings:\n",
        "            raise ValueError(\"Documents and their embeddings are not loaded.\")\n",
        "\n",
        "        query_embedding = self.embeddings.embed_query(query)\n",
        "        similarities = [\n",
        "            np.dot(query_embedding, doc_emb)\n",
        "            / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
        "            for doc_emb in self.doc_embeddings\n",
        "        ]\n",
        "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
        "        return [self.docs[i] for i in top_k_indices], top_k_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4de5fac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Recall@10 and MAP: 100%|██████████| 916/916 [1:04:58<00:00,  4.26s/it]  \n"
          ]
        }
      ],
      "source": [
        "test_ctis = list(cti_yara_eval_data.keys())\n",
        "\n",
        "r_at_10_list = []\n",
        "map_list = []\n",
        "\n",
        "rag = RAG()\n",
        "rag.load_documents(consolidated_dummy_yara_rules)\n",
        "\n",
        "for cti in tqdm(test_ctis, desc=\"Evaluating Recall@10 and MAP\"):\n",
        "    try:\n",
        "        gt_rules = cti_yara_eval_data[cti]\n",
        "        top_docs, top_indices = rag.get_top_k_docs(cti, k=10)\n",
        "\n",
        "        # Match indices of GT rules in dummy rules\n",
        "        gt_indices = set(map_subset_indices(consolidated_dummy_yara_rules, gt_rules).values())\n",
        "\n",
        "        # R@10: At least one GT in top 10\n",
        "        retrieved_set = set(top_indices)\n",
        "        hit = len(gt_indices.intersection(retrieved_set)) > 0\n",
        "        r_at_10_list.append(1.0 if hit else 0.0)\n",
        "\n",
        "        # MAP: Compute average precision for this query\n",
        "        ap = 0.0\n",
        "        hits = 0\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            if idx in gt_indices:\n",
        "                hits += 1\n",
        "                ap += hits / (i + 1)\n",
        "        ap = ap / len(gt_indices) if gt_indices else 0\n",
        "        map_list.append(ap)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing CTI: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea4177a",
      "metadata": {},
      "source": [
        "### Top - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e9161330",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Retrieval Evaluation for RAGAS Retriever ===\n",
            "Recall@10: 0.6900\n",
            "MAP:       0.2060\n",
            "Recall@10 Std Dev: 0.4628\n",
            "MAP Std Dev:       0.2118\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "\n",
        "print(\"\\n=== Retrieval Evaluation for RAGAS Retriever ===\")\n",
        "print(f\"Recall@10: {sum(r_at_10_list) / len(r_at_10_list):.4f}\")\n",
        "print(f\"MAP:       {sum(map_list) / len(map_list):.4f}\")\n",
        "print(f\"Recall@10 Std Dev: {statistics.stdev(r_at_10_list):.4f}\")\n",
        "print(f\"MAP Std Dev:       {statistics.stdev(map_list):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08a37d2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
