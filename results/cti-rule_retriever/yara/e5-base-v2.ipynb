{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "745c7ad1",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1740d57e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ceed0e",
      "metadata": {},
      "source": [
        "# Path Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5ed706f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
        "project_base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dce6fdad",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/ANONYMOUS/projects/FALCON/data/evaluation/cti-rule/yara/cti_yara_eval_data.pkl'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cti_yara_eval_data_path = os.path.join(project_base_path, \"data/evaluation/cti-rule/yara/cti_yara_eval_data.pkl\")\n",
        "cti_yara_eval_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053fdc9f",
      "metadata": {},
      "source": [
        "# Misc Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f1fa95b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_from_pickle(file_path):\n",
        "    \"\"\"\n",
        "    Loads data from a pickle file.\n",
        "\n",
        "    :param file_path: Path to the pickle file\n",
        "    :return: Loaded data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from pickle: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be1ce081",
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_subset_indices(full_list, subset_list):\n",
        "    \"\"\"\n",
        "    Maps each string in the subset list to its index in the full list.\n",
        "\n",
        "    Args:\n",
        "        full_list (list of str): The complete list of strings.\n",
        "        subset_list (list of str): A subset of strings present in the full list.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with subset strings as keys and their indices in the full list as values.\n",
        "    \"\"\"\n",
        "    index_map = {}\n",
        "    for item in subset_list:\n",
        "        try:\n",
        "            index_map[item] = full_list.index(item)\n",
        "        except ValueError:\n",
        "            # Just in case the subset contains a string not found in full_list\n",
        "            index_map[item] = -1\n",
        "    return index_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "27d32c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_topk_match(gt_indices, sorted_pred_indices, top_k):\n",
        "    top_k_preds = set(sorted_pred_indices[:top_k])\n",
        "    matched = top_k_preds.intersection(set(gt_indices))\n",
        "    return 100 * len(matched) / len(gt_indices) if gt_indices else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "81be3a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reciprocal_rank(gt_indices, sorted_pred_indices):\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            return 1.0 / rank\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c2ff1f9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_precision(gt_indices, sorted_pred_indices):\n",
        "    hits, score = 0, 0.0\n",
        "    for rank, idx in enumerate(sorted_pred_indices, start=1):\n",
        "        if idx in gt_indices:\n",
        "            hits += 1\n",
        "            score += hits / rank\n",
        "    return score / len(gt_indices) if gt_indices else 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8981d174",
      "metadata": {},
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8813b9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ⚙️ Config\n",
        "MODEL_NAME = \"/data/common/models/intfloat/e5-base-v2\"\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "###########################\n",
        "MAX_LEN = 512\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():  \n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "346b52a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bi-Encoder Model\n",
        "class SentenceEncoder(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "        return nn.functional.normalize(embeddings, p=2, dim=1)  # Normalize for cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7aca813f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model\n",
        "model = SentenceEncoder(MODEL_NAME).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffce733",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa929272",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1315c3f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "916\n"
          ]
        }
      ],
      "source": [
        "# Load the data back from the pickle file\n",
        "cti_yara_eval_data = load_from_pickle(cti_yara_eval_data_path)\n",
        "print(len(cti_yara_eval_data.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ae3973",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b774db9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "consolidated_dummy_yara_rules = []\n",
        "for cti, rules in cti_yara_eval_data.items():\n",
        "    consolidated_dummy_yara_rules.extend(rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8aff5748",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5106"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(consolidated_dummy_yara_rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9867e6",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "179fc97a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batched_embeddings(model, tokenizer, texts, batch_size):\n",
        "    \"\"\"Tokenizes and embeds texts in batches to avoid CUDA OOM.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        tokens = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
        "        input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_emb = model(input_ids, attention_mask)\n",
        "            embeddings.append(batch_emb)\n",
        "\n",
        "        # Free up memory\n",
        "        del input_ids, attention_mask, batch_emb\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3d0b313a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating CTI-YARA Semantic Scorer: 100%|██████████| 916/916 [2:30:40<00:00,  9.87s/it]  \n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "\n",
        "test_ctis = list(cti_yara_eval_data.keys())\n",
        "total_recall, total_map = 0, 0\n",
        "recall_k_list = []\n",
        "map_score_list = []\n",
        "\n",
        "for cti in tqdm(test_ctis, desc=\"Evaluating CTI-YARA Semantic Scorer\"):\n",
        "    result_idx = map_subset_indices(consolidated_dummy_yara_rules, cti_yara_eval_data[cti])\n",
        "    gt_indices = list(result_idx.values())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode CTI (single item)\n",
        "        tokenized_cti = tokenizer(cti, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
        "        input_ids_cti = tokenized_cti['input_ids'].to(DEVICE)\n",
        "        attention_mask_cti = tokenized_cti['attention_mask'].to(DEVICE)\n",
        "        emb_cti = model(input_ids_cti, attention_mask_cti)\n",
        "\n",
        "        # Encode YARA rules in batches\n",
        "        emb_dummy_rules = get_batched_embeddings(model, tokenizer, consolidated_dummy_yara_rules, batch_size)\n",
        "\n",
        "        # Compute similarity\n",
        "        dot_product_matrix = torch.matmul(emb_cti, emb_dummy_rules.T)  # shape: (1, N)\n",
        "        similarity_scores = dot_product_matrix[0]\n",
        "        sorted_indices = torch.argsort(similarity_scores, descending=True).tolist()\n",
        "\n",
        "        # Metrics\n",
        "        k = len(gt_indices)\n",
        "        recall_k = evaluate_topk_match(gt_indices, sorted_indices, 10)\n",
        "        recall_k_list.append(recall_k)\n",
        "\n",
        "        map_score = average_precision(gt_indices, sorted_indices)\n",
        "        map_score_list.append(map_score)\n",
        "\n",
        "        total_recall += recall_k\n",
        "        total_map += map_score\n",
        "\n",
        "    # Free memory\n",
        "    del emb_cti, emb_dummy_rules, dot_product_matrix\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9226c98",
      "metadata": {},
      "source": [
        "### Top - k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887e4900",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347bf324",
      "metadata": {},
      "source": [
        "### Top - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "89e00048",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Evaluation Results ===\n",
            "Average Recall@K: 19.35%\n",
            "Mean Average Precision (MAP): 0.1384\n",
            "Recall@K Standard Deviation: 25.3735\n",
            "MAP Standard Deviation: 0.1947\n"
          ]
        }
      ],
      "source": [
        "# Aggregated Scores\n",
        "n = len(test_ctis)\n",
        "print(\"\\n=== Overall Evaluation Results ===\")\n",
        "print(f\"Average Recall@K: {total_recall / n:.2f}%\")\n",
        "print(f\"Mean Average Precision (MAP): {total_map / n:.4f}\")\n",
        "\n",
        "recall_std = statistics.stdev(recall_k_list)\n",
        "map_std = statistics.stdev(map_score_list)\n",
        "print(f\"Recall@K Standard Deviation: {recall_std:.4f}\")\n",
        "print(f\"MAP Standard Deviation: {map_std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80c0ea1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
